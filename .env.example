# Cache Configs
CACHE_STORE=database # Defaults to database. Other available cache store: redis and filesystem

POSTGRES_URL=
# AI Model API Keys
OPENAI_API_KEY=                 # OpenAI API key, starting with sk-
OPENAI_API_URL=                 # OpenAI API Endpoint (optional), Default: https://api.openai.com/v1
SMALL_OPENAI_MODEL=             # Default: gpt-4o-mini
MEDIUM_OPENAI_MODEL=            # Default: gpt-4o
LARGE_OPENAI_MODEL=             # Default: gpt-4o
EMBEDDING_OPENAI_MODEL=         # Default: text-embedding-3-small
IMAGE_OPENAI_MODEL=             # Default: dall-e-3

# Eternal AI's Decentralized Inference API
ETERNALAI_URL=
ETERNALAI_MODEL=                # Default: "neuralmagic/Meta-Llama-3.1-405B-Instruct-quantized.w4a16"
ETERNALAI_API_KEY=

GROK_API_KEY=                   # GROK API Key
GROQ_API_KEY=                   # Starts with gsk_
OPENROUTER_API_KEY=
GOOGLE_GENERATIVE_AI_API_KEY=   # Gemini API key

ALI_BAILIAN_API_KEY=            # Ali Bailian API Key
NANOGPT_API_KEY=                # NanoGPT API Key

HYPERBOLIC_API_KEY=             # Hyperbolic API Key
HYPERBOLIC_MODEL=
IMAGE_HYPERBOLIC_MODEL=         # Default: FLUX.1-dev
SMALL_HYPERBOLIC_MODEL=         # Default: meta-llama/Llama-3.2-3B-Instruct
MEDIUM_HYPERBOLIC_MODEL=        # Default: meta-llama/Meta-Llama-3.1-70B-Instruct
LARGE_HYPERBOLIC_MODEL=         # Default: meta-llama/Meta-Llama-3.1-405-Instruct

# Livepeer configuration
LIVEPEER_GATEWAY_URL=           # Free inference gateways and docs: https://livepeer-eliza.com/
LIVEPEER_IMAGE_MODEL=           # Default: ByteDance/SDXL-Lightning

# Speech Synthesis
ELEVENLABS_XI_API_KEY=          # API key from elevenlabs

# Direct Client Setting
EXPRESS_MAX_PAYLOAD= # Default: 100kb

# ElevenLabs Settings
ELEVENLABS_MODEL_ID=eleven_multilingual_v2
ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM
ELEVENLABS_VOICE_STABILITY=0.5
ELEVENLABS_VOICE_SIMILARITY_BOOST=0.9
ELEVENLABS_VOICE_STYLE=0.66
ELEVENLABS_VOICE_USE_SPEAKER_BOOST=false
ELEVENLABS_OPTIMIZE_STREAMING_LATENCY=4
ELEVENLABS_OUTPUT_FORMAT=pcm_16000

# Post Interval Settings (in minutes)
POST_INTERVAL_MIN=              # Default: 90
POST_INTERVAL_MAX=              # Default: 180
POST_IMMEDIATELY=

# Post Image Interval Settings (in minutes)
POST_IMAGE_INTERVAL_MIN=        # Default: 120
POST_IMAGE_INTERVAL_MAX=        # Default: 360
POST_IMMEDIATELY_IMAGE=

# Twitter action processing configuration
ACTION_INTERVAL=300000      # Interval in milliseconds between action processing runs (default: 5 minutes)
ENABLE_ACTION_PROCESSING=true   # Set to true to enable the action processing loop

# Feature Flags
IMAGE_GEN=                      # Set to TRUE to enable image generation
USE_OPENAI_EMBEDDING=           # Set to TRUE for OpenAI/1536, leave blank for local
USE_OLLAMA_EMBEDDING=           # Set to TRUE for OLLAMA/1024, leave blank for local

# OpenRouter Models
OPENROUTER_MODEL=               # Default: uses hermes 70b/405b
SMALL_OPENROUTER_MODEL=
MEDIUM_OPENROUTER_MODEL=
LARGE_OPENROUTER_MODEL=

# REDPILL Configuration
# https://docs.red-pill.ai/get-started/supported-models
REDPILL_API_KEY=                # REDPILL API Key
REDPILL_MODEL=
SMALL_REDPILL_MODEL=            # Default: gpt-4o-mini
MEDIUM_REDPILL_MODEL=           # Default: gpt-4o
LARGE_REDPILL_MODEL=            # Default: gpt-4o

# Grok Configuration
SMALL_GROK_MODEL=       # Default: grok-2-1212
MEDIUM_GROK_MODEL=      # Default: grok-2-1212
LARGE_GROK_MODEL=       # Default: grok-2-1212
EMBEDDING_GROK_MODEL=   # Default: grok-2-1212

# Ollama Configuration
OLLAMA_SERVER_URL=              # Default: localhost:11434
OLLAMA_MODEL=
OLLAMA_EMBEDDING_MODEL=         # Default: mxbai-embed-large
SMALL_OLLAMA_MODEL=             # Default: llama3.2
MEDIUM_OLLAMA_MODEL=            # Default: hermes3
LARGE_OLLAMA_MODEL=             # Default: hermes3:70b

# Google Configuration
GOOGLE_MODEL=
SMALL_GOOGLE_MODEL=             # Default: gemini-1.5-flash-latest
MEDIUM_GOOGLE_MODEL=            # Default: gemini-1.5-flash-latest
LARGE_GOOGLE_MODEL=             # Default: gemini-1.5-pro-latest
EMBEDDING_GOOGLE_MODEL=         # Default: text-embedding-004

# Groq Configuration
SMALL_GROQ_MODEL=               # Default: llama-3.1-8b-instant
MEDIUM_GROQ_MODEL=              # Default: llama-3.3-70b-versatile
LARGE_GROQ_MODEL=               # Default: llama-3.2-90b-vision-preview
EMBEDDING_GROQ_MODEL=           # Default: llama-3.1-8b-instant

# LlamaLocal Configuration
LLAMALOCAL_PATH=                # Default: "" which is the current directory in plugin-node/dist/ which gets destroyed and recreated on every build

# NanoGPT Configuration
SMALL_NANOGPT_MODEL=            # Default: gpt-4o-mini
MEDIUM_NANOGPT_MODEL=           # Default: gpt-4o
LARGE_NANOGPT_MODEL=            # Default: gpt-4o

# Anthropic Configuration
ANTHROPIC_API_KEY=              # For Claude
SMALL_ANTHROPIC_MODEL=          # Default: claude-3-haiku-20240307
MEDIUM_ANTHROPIC_MODEL=         # Default: claude-3-5-sonnet-20241022
LARGE_ANTHROPIC_MODEL=          # Default: claude-3-5-sonnet-20241022

# Heurist Configuration
HEURIST_API_KEY=                # Get from https://heurist.ai/dev-access
SMALL_HEURIST_MODEL=            # Default: meta-llama/llama-3-70b-instruct
MEDIUM_HEURIST_MODEL=           # Default: meta-llama/llama-3-70b-instruct
LARGE_HEURIST_MODEL=            # Default: meta-llama/llama-3.1-405b-instruct
HEURIST_IMAGE_MODEL=            # Default: PepeXL

# Gaianet Configuration
GAIANET_MODEL=
GAIANET_SERVER_URL=

SMALL_GAIANET_MODEL=            # Default: llama3b
SMALL_GAIANET_SERVER_URL=       # Default: https://llama3b.gaia.domains/v1
MEDIUM_GAIANET_MODEL=           # Default: llama
MEDIUM_GAIANET_SERVER_URL=      # Default: https://llama8b.gaia.domains/v1
LARGE_GAIANET_MODEL=            # Default: qwen72b
LARGE_GAIANET_SERVER_URL=       # Default: https://qwen72b.gaia.domains/v1

GAIANET_EMBEDDING_MODEL=
USE_GAIANET_EMBEDDING=          # Set to TRUE for GAIANET/768, leave blank for local

# EVM
EVM_PRIVATE_KEY=
EVM_PROVIDER_URL=

# Fallback Wallet Configuration (deprecated)
WALLET_PRIVATE_KEY=
WALLET_PUBLIC_KEY=

# Server Configuration
SERVER_PORT=3000

# Akash Chat API Configuration docs: https://chatapi.akash.network/documentation
AKASH_CHAT_API_KEY= # Get from https://chatapi.akash.network/
SMALL_AKASH_CHAT_API_MODEL=  # Default: Meta-Llama-3-2-3B-Instruct
MEDIUM_AKASH_CHAT_API_MODEL= # Default: Meta-Llama-3-3-70B-Instruct
LARGE_AKASH_CHAT_API_MODEL=  # Default: Meta-Llama-3-1-405B-Instruct-FP8

# AWS S3 Configuration Settings for File Upload
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=
AWS_S3_BUCKET=
AWS_S3_UPLOAD_PATH=